{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project - PySpark Read\n",
    "### Authors: \n",
    "Nadav Oren 316084599\n",
    "\n",
    "Yarden Mizrahi 209521293\n",
    "\n",
    "Michaella Ichak 209085422"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import  Consumer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, when, col\n",
    "from pyspark.sql.types import StructType, StructField,StringType, DoubleType, IntegerType, ArrayType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Const Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = 'crop_data'\n",
    "LABEL_INDEX_DATAFRAME_PATH = \"label_index.csv\"\n",
    "PROCESSED_CROP_DATA_PATH = \"processed_crop_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Consumer - PySpark stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w0j98swfFouL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 18:02:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CropOptimalConditionStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "### Define Kafka source\n",
    "# Define Schema\n",
    "cropSchema = StructType([\n",
    "    StructField(\"N\",IntegerType(), True),\n",
    "    StructField(\"P\",IntegerType(),True),\n",
    "    StructField(\"K\",IntegerType(),True),\n",
    "    StructField(\"temperature\",DoubleType(),True),\n",
    "    StructField(\"humidity\",DoubleType(),True),\n",
    "    StructField(\"ph\",DoubleType(),True),\n",
    "    StructField(\"rainfall\",DoubleType(),True),\n",
    "    StructField(\"label\",StringType(),True)\n",
    "])\n",
    "\n",
    "cropSchema_array = ArrayType(cropSchema)\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .load()\n",
    "\n",
    "kafka_df_string = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Assuming the value is in JSON format, convert it to a string\n",
    "# Apply the schema to parse the JSON data\n",
    "json_df = kafka_df_string.withColumn(\"jsonData\", from_json(col(\"value\"), cropSchema_array))\n",
    "\n",
    "processed_df = json_df.selectExpr(\"explode(jsonData) as jsonData\")\n",
    "\n",
    "# Select the parsed data into a structured DataFrame\n",
    "parsed_df = processed_df.select(\"jsonData.*\")\n",
    "\n",
    "### parsed_df = main data recievied ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data and Preprocessing\n",
    "note: trying the pyspark split instead of scklit learn split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 18:10:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4a807793-5af0-4ed8-90e9-ca16d68d6093. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/08/27 18:10:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/27 18:10:08 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/08/27 18:10:08 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/08/27 18:10:08 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/08/27 18:10:08 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/08/27 18:10:08 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 1: Load the existing label-to-index mapping from a CSV file\n",
    "if not os.path.exists(LABEL_INDEX_DATAFRAME_PATH):\n",
    "    label_index_df = spark.createDataFrame(\n",
    "    [(\"rice\",0)], schema=[\"label\",\"index\"])\n",
    "else:\n",
    "    label_index_df = spark.read.csv(LABEL_INDEX_DATAFRAME_PATH, header=True, inferSchema=True)\n",
    "label_to_index = {row['label']: row['index'] for row in label_index_df.collect()}\n",
    "next_index = max(label_to_index.values()) + 1 if label_to_index else 0\n",
    "\n",
    "# Function to process each batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    global label_to_index, next_index\n",
    "\n",
    "    # Step 2: Extract distinct labels from the current batch\n",
    "    new_labels = batch_df.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Update the label-to-index mapping\n",
    "    for label in new_labels:\n",
    "        if label not in label_to_index:\n",
    "            label_to_index[label] = next_index\n",
    "            next_index += 1\n",
    "\n",
    "    # Convert the updated mapping back to a DataFrame\n",
    "    new_label_index_df = spark.createDataFrame(\n",
    "        [(label, index) for label, index in label_to_index.items()],\n",
    "        schema=[\"label\", \"index\"]\n",
    "    )\n",
    "\n",
    "    # Step 3: Manually transform the labels into indices\n",
    "    mapping_expr = when(col(\"label\").isNull(), -1)\n",
    "    for label, index in label_to_index.items():\n",
    "        mapping_expr = mapping_expr.when(col(\"label\") == label, index)\n",
    "    \n",
    "    indexed_df = batch_df.withColumn(\"label_index\", mapping_expr)\n",
    "\n",
    "    # Step 4: Save the processed data to a CSV file\n",
    "    indexed_df.write.mode(\"append\").option(\"header\",\"true\").csv(PROCESSED_CROP_DATA_PATH)\n",
    "\n",
    "    # Step 5: Update the label-to-index CSV file\n",
    "    new_label_index_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(LABEL_INDEX_DATAFRAME_PATH)\n",
    "process_data_query = parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "process_data_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
